# Vision Assist App



## ğŸ§  Description

The **Vision Assist App** empowers individuals with visual impairments by providing intelligent tools to help them better understand and navigate their surroundings. Leveraging machine learning, computer vision, and accessibility-first design, the app aims to enhance independence and quality of life.

---

## âœ¨ Features

- ğŸ¯ **Object Recognition**: Detect and announce objects in real time using the camera.
- ğŸ§­ **Navigation Assistance**: Provide basic directional or environmental awareness.
- ğŸ™ï¸ **Voice Commands**: Hands-free interaction for seamless control.
- ğŸ”Š **Text-to-Speech**: Speak out detected objects and environment info.

---

## ğŸ“² Installation

To install and run the app locally, follow these steps:

```bash
git clone https://github.com/spandanavangapandu/vision-assist-mobile-app.git
cd vision-assist-mobile-app
```
1. Open the project in Android Studio.
2. Connect an Android device or start an emulator.
3. Click Run â–¶ï¸ in Android Studio.

---

## ğŸ§° Tech Stack
- **Language**: Kotlin / Java
- **ML Frameworks**: TensorFlow Lite, ML Kit
- **Camera API**: CameraX
- **Other**: Android Jetpack Libraries, Text-to-Speech

---

## ğŸ§ª Machine Learning Model
The app uses a pre-trained model (model.tflite) for object recognition. Place it under:
```
app/src/main/assets/
```
Include any label files (e.g., labels.txt) in the same directory.

---

## ğŸ” Accessibility Focus
We prioritize usability for visually impaired users:
- Audio feedback for detected objects.
- Minimal-touch UI.
- Voice-command integration.

---

## ğŸ§‘â€ğŸ’» Contributing
We welcome community contributions! To contribute, follow these steps:
1. Fork the repository.
2. Create a new branch: `git checkout -b feature-name`.
3. Commit your changes: `git commit -m "Added new feature"`.
4. Push to your fork: `git push origin feature-name`.
5. Submit a Pull Request ğŸ™Œ.

---



## Author
**[SPANDANA VANGAPANDU]**

## For any queries, mail: 
**spandanavangapandu@gmail.com**
